# NLP

Part of my work for Kaggle's Toxic Comment Classification Challenge. Positioned 15th out of 4551 competitors

Shortform explanation used below:
<br />
BiLSTM: Bidirectional Long Short Term Memory
<br />
W2V: Gensim Word2Vec
<br />
attention: attention mechanism
<br />
DPCNN: Deep pyramid convolutional neural network
<br />
300d: Glove embeddings of 300d
<br />
logreg: logistic regression
<br />
prepost: Two different embeddings trained for initial part of sentence and for the end of sentence to counter the problems 
faced by RNNs when the sequence length becomes much larger
<br />

The titles of below files will now be self explanatory
<br />

Files:
<br />
BiLSTM-W2V.ipynb
<br />
DPCNN.ipynb
<br />
Deepemoji_basic.ipynb
<br />
attention_300d.ipynb
<br />
bi_gru_post.ipynb
<br />
bi_gru_post_features.ipynb
<br />
bi_post.ipynb
<br />
cnn_demoji_attention(1).ipynb
<br />
cnn_gru.ipynb
<br />
logreg2.py
<br />
pre_post_deepemoji.ipynb
<br />
svm.ipynb
<br />
