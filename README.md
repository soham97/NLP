# NLP
NLP

Part of my work for Kaggle's Toxic Comment Classification Challenge. Positioned 15th out of 4551 competitors

Shortform explanation used below:
BiLSTM: Bidirectional Long Short Term Memory
W2V: Gensim Word2Vec
attention: attention mechanism
DPCNN: Deep pyramid convolutional neural network
300d: Glove embeddings of 300d
logreg: logistic regression
prepost: Two different embeddings trained for initial part of sentence and for the end of sentence to counter the problems faced by RNNs when the sequence length becomes much larger

The titles of below files will now be self explanatory

Files:
BiLSTM-W2V.ipynb
DPCNN.ipynb
Deepemoji_basic.ipynb
attention_300d.ipynb
bi_gru_post.ipynb
bi_gru_post_features.ipynb
bi_post.ipynb
cnn_demoji_attention(1).ipynb
cnn_gru.ipynb
logreg2.py
pre_post_deepemoji.ipynb
svm.ipynb
